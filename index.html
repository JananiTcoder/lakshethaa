<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>AR Image Replacement</title>
<script async src="https://docs.opencv.org/4.x/opencv.js"></script>
<style>
  body { margin:0; background:black; overflow:hidden; }
  #camera, #canvas {
    position:fixed; inset:0;
    width:100vw; height:100vh; object-fit:cover;
  }
  #videoSource, #videoCanvas, #referenceImg { display:none; }
  #debug {
    position:fixed; left:8px; top:8px;
    color:#0f0; font-family:monospace; font-size:12px;
    background:rgba(0,0,0,0.5); padding:4px 6px; border-radius:4px;
  }
</style>
</head>
<body>

<video id="camera" autoplay playsinline></video>
<canvas id="canvas"></canvas>

<video id="videoSource" src="output.mp4" playsinline muted></video>
<canvas id="videoCanvas"></canvas>
<img id="referenceImg" src="reference.jpg">

<div id="debug">auto AR, muted</div>

<script>
const camera = document.getElementById('camera');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const videoSource = document.getElementById('videoSource');
const videoCanvas = document.getElementById('videoCanvas');
const videoCtx = videoCanvas.getContext('2d');
const refImg = document.getElementById('referenceImg');
const debugEl = document.getElementById('debug');

let videoReady = false;

// Start camera
async function startCamera() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: { exact:"environment" }, width:{ideal:1280}, height:{ideal:720} },
      audio: false
    });
    camera.srcObject = stream;
  } catch (e) {
    const stream = await navigator.mediaDevices.getUserMedia({ video:true, audio:false });
    camera.srcObject = stream;
  }
}
startCamera();

videoSource.addEventListener('loadedmetadata', () => {
  videoReady = true;
});

cv['onRuntimeInitialized'] = () => {
  const refMat = cv.imread(refImg);
  const refGray = new cv.Mat();
  cv.cvtColor(refMat, refGray, cv.COLOR_BGR2GRAY);

  const orb = new cv.ORB();
  const kpRef = new cv.KeyPointVector();
  const descRef = new cv.Mat();
  orb.detectAndCompute(refGray, new cv.Mat(), kpRef, descRef);

  const bf = new cv.BFMatcher(cv.NORM_HAMMING);

  function process() {
    if (camera.readyState !== 4) { requestAnimationFrame(process); return; }

    canvas.width = camera.videoWidth;
    canvas.height = camera.videoHeight;

    ctx.drawImage(camera, 0, 0, canvas.width, canvas.height);
    let frame = cv.imread(canvas);

    const grayFrame = new cv.Mat();
    cv.cvtColor(frame, grayFrame, cv.COLOR_BGR2GRAY);

    const kpFrame = new cv.KeyPointVector();
    const descFrame = new cv.Mat();
    orb.detectAndCompute(grayFrame, new cv.Mat(), kpFrame, descFrame);

    let detected = false;
    let goodCount = 0;

    if (!descFrame.empty()) {
      const matches = new cv.DMatchVector();
      bf.match(descRef, descFrame, matches);

      const goodMatches = [];
      for (let i = 0; i < matches.size(); i++) {
        const d = matches.get(i).distance;
        if (d < 80) goodMatches.push(matches.get(i));
      }
      goodCount = goodMatches.length;

      if (goodMatches.length >= 8 && videoReady) {
        const srcPts = [];
        const dstPts = [];
        for (let i = 0; i < goodMatches.length; i++) {
          const m = goodMatches[i];
          const pRef = kpRef.get(m.queryIdx).pt;
          const pFrame = kpFrame.get(m.trainIdx).pt;
          srcPts.push(pRef.x, pRef.y);
          dstPts.push(pFrame.x, pFrame.y);
        }

        const srcMat = cv.matFromArray(goodMatches.length, 1, cv.CV_32FC2, srcPts);
        const dstMat = cv.matFromArray(goodMatches.length, 1, cv.CV_32FC2, dstPts);
        const mask = new cv.Mat();
        const H = cv.findHomography(srcMat, dstMat, cv.RANSAC, 5, mask);

        if (!H.empty()) {
          detected = true;

          const vw = videoSource.videoWidth || 640;
          const vh = videoSource.videoHeight || 360;
          videoCanvas.width = vw;
          videoCanvas.height = vh;
          videoCtx.drawImage(videoSource, 0, 0, vw, vh);
          const vidFrame = cv.imread(videoCanvas);

          const warped = new cv.Mat();
          const dsize = new cv.Size(frame.cols, frame.rows);
          cv.warpPerspective(vidFrame, warped, H, dsize);

          const maskWarped = new cv.Mat();
          cv.cvtColor(warped, maskWarped, cv.COLOR_BGR2GRAY);
          cv.threshold(maskWarped, maskWarped, 1, 255, cv.THRESH_BINARY);

          warped.copyTo(frame, maskWarped);

          vidFrame.delete();
          warped.delete();
          maskWarped.delete();
        }

        H.delete();
        srcMat.delete();
        dstMat.delete();
        mask.delete();
      }
      matches.delete();
    }

    debugEl.textContent =
      `detected=${detected} goodMatches=${goodCount}`;

    if (detected) {
      if (videoSource.paused) {
        videoSource.play().catch(() => {});
      }
    } else {
      if (!videoSource.paused) {
        videoSource.pause();
        videoSource.currentTime = 0;
      }
    }

    cv.imshow(canvas, frame);

    frame.delete();
    grayFrame.delete();
    kpFrame.delete();
    descFrame.delete();

    requestAnimationFrame(process);
  }

  process();
};
</script>
</body>
</html>
