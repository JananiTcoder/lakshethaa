<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>AR Image Replacement</title>
<script async src="https://docs.opencv.org/4.x/opencv.js"></script>
<style>
  body { margin:0; background:black; overflow:hidden; }
  #camera, #canvas {
    position:fixed; inset:0;
    width:100vw; height:100vh; object-fit:cover;
  }
  #videoSource, #videoCanvas, #referenceImg { display:none; }
</style>
</head>
<body>

<video id="camera" autoplay playsinline></video>
<canvas id="canvas"></canvas>

<video id="videoSource" src="output.mp4" playsinline></video>
<canvas id="videoCanvas"></canvas>
<img id="referenceImg" src="reference.jpg">

<script>
const camera = document.getElementById('camera');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const videoSource = document.getElementById('videoSource');
const videoCanvas = document.getElementById('videoCanvas');
const videoCtx = videoCanvas.getContext('2d');
const refImg = document.getElementById('referenceImg');

// flag: has user allowed audio?
let audioUnlocked = false;

// Start camera
async function startCamera() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: { exact:"environment" }, width:{ideal:1280}, height:{ideal:720} },
      audio: false
    });
    camera.srcObject = stream;
  } catch (e) {
    const stream = await navigator.mediaDevices.getUserMedia({ video:true, audio:false });
    camera.srcObject = stream;
  }
}
startCamera();

// One user gesture to unlock audio, but do NOT start playing
document.body.addEventListener('click', () => {
  audioUnlocked = true;
  // try a silent play & immediately pause to satisfy autoplay policy
  videoSource.muted = true;
  videoSource.play().then(() => {
    videoSource.pause();
    videoSource.currentTime = 0;
    videoSource.muted = false;
  }).catch(()=>{});
}, { once:true });

cv['onRuntimeInitialized'] = () => {
  // Prepare reference image
  const refMat = cv.imread(refImg);
  const refGray = new cv.Mat();
  cv.cvtColor(refMat, refGray, cv.COLOR_BGR2GRAY);

  const orb = new cv.ORB();
  const kpRef = new cv.KeyPointVector();
  const descRef = new cv.Mat();
  orb.detectAndCompute(refGray, new cv.Mat(), kpRef, descRef);

  const bf = new cv.BFMatcher(cv.NORM_HAMMING);

  function process() {
    if (camera.readyState !== 4) { requestAnimationFrame(process); return; }

    canvas.width = camera.videoWidth;
    canvas.height = camera.videoHeight;

    ctx.drawImage(camera, 0, 0, canvas.width, canvas.height);
    let frame = cv.imread(canvas);

    const grayFrame = new cv.Mat();
    cv.cvtColor(frame, grayFrame, cv.COLOR_BGR2GRAY);

    const kpFrame = new cv.KeyPointVector();
    const descFrame = new cv.Mat();
    orb.detectAndCompute(grayFrame, new cv.Mat(), kpFrame, descFrame);

    let detected = false; // assume not detected this frame

    if (!descFrame.empty()) {
      const matches = new cv.DMatchVector();
      bf.match(descRef, descFrame, matches);

      let goodMatches = [];
      for (let i = 0; i < matches.size(); i++) {
        if (matches.get(i).distance < 50) goodMatches.push(matches.get(i));
      }

      if (goodMatches.length >= 10 && videoSource.readyState >= 2) {
        const srcPts = [];
        const dstPts = [];
        for (let i = 0; i < goodMatches.length; i++) {
          const m = goodMatches[i];
          const pRef = kpRef.get(m.queryIdx).pt;
          const pFrame = kpFrame.get(m.trainIdx).pt;
          srcPts.push(pRef.x, pRef.y);
          dstPts.push(pFrame.x, pFrame.y);
        }

        const srcMat = cv.matFromArray(goodMatches.length, 1, cv.CV_32FC2, srcPts);
        const dstMat = cv.matFromArray(goodMatches.length, 1, cv.CV_32FC2, dstPts);
        const mask = new cv.Mat();
        const H = cv.findHomography(srcMat, dstMat, cv.RANSAC, 5, mask);

        if (!H.empty()) {
          detected = true;

          // draw current video frame to hidden canvas
          videoCanvas.width = videoSource.videoWidth || 640;
          videoCanvas.height = videoSource.videoHeight || 360;
          videoCtx.drawImage(videoSource, 0, 0, videoCanvas.width, videoCanvas.height);
          const vidFrame = cv.imread(videoCanvas);

          const warped = new cv.Mat();
          const dsize = new cv.Size(frame.cols, frame.rows);
          cv.warpPerspective(vidFrame, warped, H, dsize);

          const maskWarped = new cv.Mat();
          cv.cvtColor(warped, maskWarped, cv.COLOR_BGR2GRAY);
          cv.threshold(maskWarped, maskWarped, 1, 255, cv.THRESH_BINARY);

          warped.copyTo(frame, maskWarped);

          vidFrame.delete();
          warped.delete();
          maskWarped.delete();
        }

        H.delete();
        srcMat.delete();
        dstMat.delete();
        mask.delete();
      }
      matches.delete();
    }

    // control video playback based on detection
    if (detected && audioUnlocked) {
      if (videoSource.paused) {
        videoSource.play().catch(()=>{});
      }
    } else {
      if (!videoSource.paused) {
        videoSource.pause();
        videoSource.currentTime = 0; // restart from beginning next time
      }
    }

    cv.imshow(canvas, frame);

    frame.delete();
    grayFrame.delete();
    kpFrame.delete();
    descFrame.delete();

    requestAnimationFrame(process);
  }

  process();
};
</script>
</body>
</html>
