<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Image Replace With Video</title>

<script async src="https://docs.opencv.org/4.x/opencv.js"></script>

<style>
  body {
    margin: 0;
    background: black;
    overflow: hidden;
  }

  #cameraView {
    width: 100vw;
    height: 100vh;
    object-fit: cover;
  }

  #overlay {
    position: absolute;
    top: 30%;
    left: 50%;
    transform: translateX(-50%);
    width: 300px;
    height: 200px;
  }

  #targetImg, #resultVideo {
    width: 100%;
    height: 100%;
  }

  #resultVideo {
    display: none;
  }

  #camera, #canvas {
    display: none;
  }
</style>
</head>
<body>

<!-- Camera -->
<video id="cameraView" autoplay playsinline muted></video>

<!-- Overlay -->
<div id="overlay">
  <img id="targetImg" src="reference.jpg">
  <video id="resultVideo" src="output.mp4" playsinline></video>
</div>

<!-- Hidden processing -->
<video id="camera" autoplay playsinline></video>
<canvas id="canvas"></canvas>

<script>
const camera = document.getElementById("camera");
const cameraView = document.getElementById("cameraView");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");

const targetImg = document.getElementById("targetImg");
const resultVideo = document.getElementById("resultVideo");

let detected = false;

/* ---------- FORCE BACK CAMERA ---------- */
async function startCamera() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: { exact: "environment" } },
      audio: false
    });
    camera.srcObject = stream;
    cameraView.srcObject = stream;
  } catch {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: "environment" },
      audio: false
    });
    camera.srcObject = stream;
    cameraView.srcObject = stream;
  }
}

startCamera();

/* ---------- OPENCV ---------- */
cv.onRuntimeInitialized = () => {
  console.log("OpenCV ready");

  let refGray = new cv.Mat();
  let refColor = cv.imread(targetImg);
  cv.cvtColor(refColor, refGray, cv.COLOR_BGR2GRAY);

  let orb = new cv.ORB(1000);
  let refKP = new cv.KeyPointVector();
  let refDesc = new cv.Mat();

  orb.detectAndCompute(refGray, new cv.Mat(), refKP, refDesc);

  console.log("Reference keypoints:", refKP.size());

  function detect() {
    if (detected) return;
    if (camera.readyState !== 4) {
      requestAnimationFrame(detect);
      return;
    }

    canvas.width = camera.videoWidth * 0.5;
    canvas.height = camera.videoHeight * 0.5;
    ctx.drawImage(camera, 0, 0, canvas.width, canvas.height);

    let frame = cv.imread(canvas);
    let gray = new cv.Mat();
    cv.cvtColor(frame, gray, cv.COLOR_BGR2GRAY);

    let kp = new cv.KeyPointVector();
    let desc = new cv.Mat();

    orb.detectAndCompute(gray, new cv.Mat(), kp, desc);

    if (!desc.empty()) {
      let bf = new cv.BFMatcher(cv.NORM_HAMMING);
      let matches = new cv.DMatchVector();

      bf.match(refDesc, desc, matches);

      let good = 0;
      for (let i = 0; i < matches.size(); i++) {
        if (matches.get(i).distance < 70) good++;
      }

      console.log("Good matches:", good);

      if (good >= 10) {
        detected = true;

        targetImg.style.display = "none";
        resultVideo.style.display = "block";
        resultVideo.muted = false;
        resultVideo.play();

        return;
      }

      matches.delete();
      bf.delete();
    }

    frame.delete();
    gray.delete();
    kp.delete();
    desc.delete();

    requestAnimationFrame(detect);
  }

  detect();
};
</script>

</body>
</html>
